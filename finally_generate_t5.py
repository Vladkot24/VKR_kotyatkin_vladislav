# -*- coding: utf-8 -*-
"""finally_generate_t5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/gist/Vladkot24/6e02ad882adf0630fe7add374a070d9e/finally_generate_t5.ipynb
"""

# 1. Отключение (размонтирование) Google Drive
from google.colab import drive
drive.flush_and_unmount()

"""1. Импортируем и скачиваем необходимые библиотеки"""

!pip install rouge-score

import os
import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import (
    T5ForConditionalGeneration,
    T5Tokenizer,
    get_linear_schedule_with_warmup
)
from torch.optim import AdamW
from sklearn.model_selection import train_test_split
import gc
from tqdm.notebook import tqdm
import warnings

"""Игнорирование предупреждений для более чистого вывода"""

# 3. Повторное подключение после перезапуска
from google.colab import drive
drive.mount('/content/drive')

warnings.filterwarnings("ignore")

import re

def simple_tokenize(text):
    # Заменяем все знаки препинания на пробелы + этот знак + пробел
    text = re.sub(r'([.,!?;:()\[\]{}«»"\'\-])', r' \1 ', text)
    # Убираем множественные пробелы
    text = re.sub(r'\s+', ' ', text)
    # Разбиваем по пробелам и возвращаем список токенов
    return text.strip().lower().split()

"""Установка воспроизводимости результатов"""

SEED = 42
torch.manual_seed(SEED)
np.random.seed(SEED)
if torch.cuda.is_available():
    torch.backends.cudnn.deterministic = True

"""2. ПОДГОТОВКА ДАННЫХ И НАСТРОЙКА ПАРАМЕТРОВ

Параметры модели и обучения
"""

MODEL_NAME = "DeepPavlov/ruT5-base"  # Русскоязычная T5-модель
MAX_SOURCE_LENGTH = 1024  # Уменьшаем длину для экономии памяти
MAX_TARGET_LENGTH = 64   # Ограничиваем длину целевого текста
BATCH_SIZE = 8          # Уменьшаем размер батча
GRADIENT_ACCUMULATION_STEPS = 2  # Увеличиваем для компенсации меньшего батча
EPOCHS = 6             # Уменьшаем для демонстрации
LEARNING_RATE = 3e-5
WARMUP_STEPS = 0
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Используемое устройство: {DEVICE}")

"""Класс для создания датасета"""

def load_and_prepare_data():
    """
    Загружает данные из CSV-файла и разделяет их на train, validation и test наборы.
    Добавлена обработка ошибок и создание тестовых данных при отсутствии файла.
    """
    # Попытка загрузки данных из файла
    try:
        csv_path = '/content/drive/MyDrive/balanced_legal_dataset.csv'
        df = pd.read_csv(csv_path)
        print(f"Данные успешно загружены из {csv_path}")
    except Exception as e:
        print(f"Ошибка при загрузке данных: {e}")
    try:
        # Проверка, что у нас есть достаточно данных для разделения
        if len(df) < 3:
            print("Недостаточно данных для разделения на три набора. Дублирую данные.")
            # Дублируем данные, чтобы было достаточно для разделения
            while len(df) < 6:
                df = pd.concat([df, df])

        train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)
        val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)

        print(f"Данные разделены: Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}")
        return train_df, val_df, test_df
    except Exception as split_error:
        print(f"Ошибка при разделении данных: {split_error}")
        # Вместо None вернем базовое разделение датафрейма
        n = len(df)
        train_size = int(n * 0.7)
        val_size = int(n * 0.15)

        train_df = df[:train_size]
        val_df = df[train_size:train_size+val_size]
        test_df = df[train_size+val_size:]

        print("Выполнено базовое разделение данных.")
        return train_df, val_df, test_df

class LegalDataset(Dataset):
    def __init__(self, texts, targets, tokenizer, max_source_len, max_target_len):
        self.texts = texts
        self.targets = targets
        self.tokenizer = tokenizer
        self.max_source_len = max_source_len
        self.max_target_len = max_target_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        target = self.targets[idx]

        # Токенизация входного текста
        source_encoding = self.tokenizer(
            text,
            max_length=self.max_source_len,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )

        # Токенизация целевого текста
        target_encoding = self.tokenizer(
            target,
            max_length=self.max_target_len,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )

        input_ids = source_encoding["input_ids"].squeeze()
        attention_mask = source_encoding["attention_mask"].squeeze()
        labels = target_encoding["input_ids"].squeeze()
        # Заменяем padding token на -100, чтобы они не учитывались при расчете потерь
        labels[labels == self.tokenizer.pad_token_id] = -100

        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels,
            "raw_text": text,
            "raw_target": target
        }

"""3. СОЗДАНИЕ МОДЕЛИ И ФУНКЦИЙ ОБУЧЕНИЯ"""

def create_model_and_tokenizer():
    print(f"Загрузка модели и токенизатора: {MODEL_NAME}")
    try:
        tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)
        model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)
        return model, tokenizer
    except Exception as e:
        print(f"Ошибка при загрузке модели: {e}")
        print("Используем меньшую русскоязычную T5 модель...")
        # Пробуем использовать другую русскоязычную модель
        try:
            alt_model_name = "cointegrated/rut5-small"  # Меньшая русскоязычная модель
            print(f"Попытка загрузки альтернативной модели: {alt_model_name}")
            tokenizer = T5Tokenizer.from_pretrained(alt_model_name)
            model = T5ForConditionalGeneration.from_pretrained(alt_model_name)
            return model, tokenizer
        except Exception as e2:
            print(f"Ошибка при загрузке альтернативной модели: {e2}")
            raise

def train_epoch(model, dataloader, optimizer, scheduler, device):
    model.train()
    total_loss = 0
    step = 0

    progress_bar = tqdm(dataloader, desc="Обучение")
    for batch_idx, batch in enumerate(progress_bar):
        try:
            # Перемещение данных на GPU, если доступно
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            # Прямой проход
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )

            loss = outputs.loss / GRADIENT_ACCUMULATION_STEPS  # Нормализуем потери

            # Обратное распространение ошибки
            loss.backward()

            # Учет потерь
            total_loss += loss.item() * GRADIENT_ACCUMULATION_STEPS  # Денормализуем для логирования

            # Обновление прогресс-бара
            progress_bar.set_postfix({"loss": f"{loss.item() * GRADIENT_ACCUMULATION_STEPS:.4f}"})

            # Выполняем оптимизацию после накопления градиентов
            if (batch_idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0 or batch_idx == len(dataloader) - 1:
                # Оптимизация
                optimizer.step()
                if scheduler is not None:
                    scheduler.step()
                # Очистка градиентов
                optimizer.zero_grad()
                step += 1

            # Очистка памяти
            del input_ids, attention_mask, labels, outputs, loss
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        except Exception as e:
            print(f"Ошибка при обработке батча: {e}")
            # Пропускаем проблемный батч
            optimizer.zero_grad()
            continue

    return total_loss / max(1, len(dataloader))

def validate(model, dataloader, device, max_batches=None):
    model.eval()
    total_loss = 0
    batch_count = 0

    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(dataloader, desc="Валидация")):
            try:
                # Ограничиваем количество батчей для валидации, если задано
                if max_batches is not None and batch_idx >= max_batches:
                    break

                input_ids = batch["input_ids"].to(device)
                attention_mask = batch["attention_mask"].to(device)
                labels = batch["labels"].to(device)

                outputs = model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )

                loss = outputs.loss
                total_loss += loss.item()
                batch_count += 1

                del input_ids, attention_mask, labels, outputs, loss
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

            except Exception as e:
                print(f"Ошибка при валидации батча {batch_idx}: {e}")
                continue

    return total_loss / max(1, batch_count)

"""4. ФУНКЦИИ ДЛЯ ОЦЕНКИ КАЧЕСТВА"""

def evaluate_predictions(predictions, targets):
    # Реализация упрощенного BLEU-1 без библиотеки NLTK
    bleu_scores = []

    for pred, target in zip(predictions, targets):
        pred_tokens = simple_tokenize(pred)
        target_tokens = simple_tokenize(target)

        # Избегаем деления на ноль
        if not pred_tokens:
            bleu_scores.append(0)
            continue

        # Считаем совпадающие униграммы
        matches = 0
        for token in pred_tokens:
            if token in target_tokens:
                matches += 1

        # Вычисляем точность (precision)
        precision = matches / len(pred_tokens) if len(pred_tokens) > 0 else 0

        # Штраф за краткость (brevity penalty)
        bp = 1.0 if len(pred_tokens) >= len(target_tokens) else np.exp(1 - len(target_tokens) / max(1, len(pred_tokens)))

        # Итоговый BLEU-1
        bleu1 = bp * precision
        bleu_scores.append(bleu1)

    # Упрощенная реализация ROUGE (без библиотеки rouge_score)
    rouge1_scores = []
    rouge2_scores = []
    rougeL_scores = []

    for pred, target in zip(predictions, targets):
        pred_tokens = simple_tokenize(pred)
        target_tokens = simple_tokenize(target)

        # Проверка на пустые списки токенов
        if not target_tokens:
            rouge1_scores.append(0)
            rouge2_scores.append(0)
            rougeL_scores.append(0)
            continue

        # ROUGE-1 (униграммы)
        rouge1_matches = sum(1 for token in pred_tokens if token in target_tokens)
        rouge1 = rouge1_matches / len(target_tokens) if len(target_tokens) > 0 else 0
        rouge1_scores.append(rouge1)

        # ROUGE-2 (биграммы)
        if len(pred_tokens) < 2 or len(target_tokens) < 2:
            rouge2_scores.append(0)
        else:
            pred_bigrams = [' '.join(pred_tokens[i:i+2]) for i in range(len(pred_tokens)-1)]
            target_bigrams = [' '.join(target_tokens[i:i+2]) for i in range(len(target_tokens)-1)]

            rouge2_matches = sum(1 for bigram in pred_bigrams if bigram in target_bigrams)
            rouge2 = rouge2_matches / len(target_bigrams) if len(target_bigrams) > 0 else 0
            rouge2_scores.append(rouge2)

        # ROUGE-L (самая длинная общая подпоследовательность)
        # Упрощённая версия: используем совпадение отдельных токенов
        rougeL = rouge1  # Упрощение: используем ROUGE-1 как приближение ROUGE-L
        rougeL_scores.append(rougeL)

    # Словесное совпадение (процент совпадающих слов)
    word_match_scores = []

    for pred, target in zip(predictions, targets):
        pred_words = set(simple_tokenize(pred))
        target_words = set(simple_tokenize(target))

        if len(target_words) > 0:
            intersection = pred_words.intersection(target_words)
            word_match = len(intersection) / len(target_words)
            word_match_scores.append(word_match)
        else:
            word_match_scores.append(0)

    results = {
        'bleu1': np.mean(bleu_scores),
        'rouge1': np.mean(rouge1_scores),
        'rouge2': np.mean(rouge2_scores),
        'rougeL': np.mean(rougeL_scores),
        'word_match': np.mean(word_match_scores)
    }

    return results

def generate_predictions(model, dataloader, tokenizer, device, num_samples=None):
    model.eval()
    predictions = []
    targets = []
    texts = []

    with torch.no_grad():
        for i, batch in enumerate(tqdm(dataloader, desc="Генерация предсказаний")):
            if num_samples is not None and i >= num_samples:
                break

            try:
                input_ids = batch["input_ids"].to(device)
                attention_mask = batch["attention_mask"].to(device)

                # Генерация прогнозов с безопасными параметрами
                generated_ids = model.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    max_length=MAX_TARGET_LENGTH,
                    num_beams=2,  # Уменьшаем для экономии памяти
                    no_repeat_ngram_size=2,
                    early_stopping=True
                )

                # Декодирование прогнозов
                pred_text = [tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids]
                predictions.extend(pred_text)

                # Сохранение оригинальных целевых текстов
                targets.extend(batch["raw_target"])
                texts.extend(batch["raw_text"])

                del input_ids, attention_mask, generated_ids
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

            except Exception as e:
                print(f"Ошибка при генерации предсказаний для батча {i}: {e}")
                # В случае ошибки добавляем пустые предсказания
                batch_size = len(batch["raw_target"])
                predictions.extend([""] * batch_size)
                targets.extend(batch["raw_target"])
                texts.extend(batch["raw_text"])
                continue

    return predictions, targets, texts

"""5. ВИЗУАЛИЗАЦИЯ РЕЗУЛЬТАТОВ"""

def plot_training_history(train_losses, val_losses):
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label='Обучающая выборка')
    plt.plot(val_losses, label='Валидационная выборка')
    plt.xlabel('Эпоха')
    plt.ylabel('Потеря')
    plt.title('История потерь при обучении')
    plt.legend()
    plt.grid(True)
    try:
        plt.savefig('training_history.png')
    except Exception as e:
        print(f"Не удалось сохранить график: {e}")
    plt.show()

def plot_evaluation_metrics(metrics_history):
    plt.figure(figsize=(12, 8))

    for metric, values in metrics_history.items():
        plt.plot(values, label=metric)

    plt.xlabel('Эпоха')
    plt.ylabel('Значение метрики')
    plt.title('Изменение метрик по эпохам')
    plt.legend()
    plt.grid(True)
    try:
        plt.savefig('metrics_history.png')
    except Exception as e:
        print(f"Не удалось сохранить график: {e}")
    plt.show()

def plot_final_metrics(final_metrics):
    plt.figure(figsize=(10, 6))
    metrics = list(final_metrics.keys())
    values = list(final_metrics.values())

    # Создание бар-чарта
    bars = plt.bar(metrics, values, color='skyblue')

    # Добавление значений над столбцами
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                 f'{height:.3f}',
                 ha='center', va='bottom', rotation=0, fontsize=9)

    plt.ylim(0, 1.0)  # Все метрики в диапазоне [0, 1]
    plt.xlabel('Метрика')
    plt.ylabel('Значение')
    plt.title('Итоговые метрики модели')
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.tight_layout()
    try:
        plt.savefig('final_metrics.png')
    except Exception as e:
        print(f"Не удалось сохранить график: {e}")
    plt.show()

def display_predictions(texts, predictions, targets, n=15):
    n = min(n, len(texts))

    for i in range(n):
        print(f"\nПример {i+1}:")
        print(f"Входной текст (сокращенный): {texts[i][:400]}...")
        print(f"Целевая резолюция: {targets[i]}")
        print(f"Предсказанная резолюция: {predictions[i]}")
        print("-" * 80)

"""6. ОСНОВНАЯ ФУНКЦИЯ ДЛЯ ОБУЧЕНИЯ И ОЦЕНКИ МОДЕЛИ"""

def train_and_evaluate():
    # Загрузка и подготовка данных
    train_df, val_df, test_df = load_and_prepare_data()

    # Создание модели и токенизатора
    try:
        model, tokenizer = create_model_and_tokenizer()
        model = model.to(DEVICE)
    except Exception as e:
        print(f"Ошибка при загрузке модели: {e}")
        print("Используем меньшую русскоязычную T5 модель...")
        # Пробуем использовать другую русскоязычную модель
        try:
            MODEL_NAME = "cointegrated/rut5-small"  # Меньшая русскоязычная модель
            tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)
            model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)
            model = model.to(DEVICE)
        except Exception as e2:
            print(f"Ошибка при загрузке альтернативной модели: {e2}")
            raise

    # Создание датасетов и даталоадеров
    train_dataset = LegalDataset(
        train_df['input_text'].tolist(),
        train_df['extracted_resolution'].tolist(),
        tokenizer,
        MAX_SOURCE_LENGTH,
        MAX_TARGET_LENGTH
    )

    val_dataset = LegalDataset(
        val_df['input_text'].tolist(),
        val_df['extracted_resolution'].tolist(),
        tokenizer,
        MAX_SOURCE_LENGTH,
        MAX_TARGET_LENGTH
    )

    test_dataset = LegalDataset(
        test_df['input_text'].tolist(),
        test_df['extracted_resolution'].tolist(),
        tokenizer,
        MAX_SOURCE_LENGTH,
        MAX_TARGET_LENGTH
    )

    train_dataloader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True
    )

    val_dataloader = DataLoader(
        val_dataset,
        batch_size=BATCH_SIZE,
        shuffle=False
    )

    test_dataloader = DataLoader(
        test_dataset,
        batch_size=BATCH_SIZE,
        shuffle=False
    )

    # Настройка оптимизатора и шедулера
    # Применяем weight decay только к определенным слоям для улучшения обучения
    no_decay = ["bias", "LayerNorm.weight"]
    optimizer_grouped_parameters = [
        {
            "params": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],
            "weight_decay": 0.01,
        },
        {
            "params": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],
            "weight_decay": 0.0,
        },
    ]
    optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE)

    # Учитываем градиентное накопление при расчете шагов
    total_steps = (len(train_dataloader) // GRADIENT_ACCUMULATION_STEPS) * EPOCHS
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=WARMUP_STEPS,
        num_training_steps=total_steps
    )

    # История обучения
    train_losses = []
    val_losses = []
    metrics_history = {
        'bleu1': [],
        'rouge1': [],
        'rouge2': [],
        'rougeL': [],
        'word_match': []
    }

    # Обучение модели
    print("\nНачало обучения модели:")
    for epoch in range(EPOCHS):
        print(f"\nЭпоха {epoch + 1}/{EPOCHS}")

        # Обучение
        train_loss = train_epoch(model, train_dataloader, optimizer, scheduler, DEVICE)
        train_losses.append(train_loss)
        print(f"Потери на обучающей выборке: {train_loss:.4f}")

        # Валидация (ограничиваем количество батчей для экономии памяти)
        val_loss = validate(model, val_dataloader, DEVICE, max_batches=1000)
        val_losses.append(val_loss)
        print(f"Потери на валидационной выборке: {val_loss:.4f}")

        # Оценка на текущей эпохе (ограничиваем количество образцов)
        try:
            predictions, targets, _ = generate_predictions(model, val_dataloader, tokenizer, DEVICE, num_samples=1000)
            metrics = evaluate_predictions(predictions, targets)
        except RuntimeError as e:
            if 'CUDA out of memory' in str(e):
                print("CUDA out of memory при генерации предсказаний. Пробуем с меньшим количеством образцов...")
                torch.cuda.empty_cache()
                predictions, targets, _ = generate_predictions(model, val_dataloader, tokenizer, DEVICE, num_samples=5)
                metrics = evaluate_predictions(predictions, targets)
            else:
                raise

        for metric, value in metrics.items():
            metrics_history[metric].append(value)
            print(f"{metric}: {value:.4f}")

        # Очистка памяти
        gc.collect()
        torch.cuda.empty_cache() if torch.cuda.is_available() else None

    # Визуализация истории обучения
    plot_training_history(train_losses, val_losses)
    plot_evaluation_metrics(metrics_history)

    # Итоговая оценка на тестовой выборке
    print("\nИтоговая оценка на тестовой выборке:")
    try:
        # Добавляем ограничение на количество образцов для тестирования,
        # чтобы избежать проблем с памятью
        num_test_samples = min(100, len(test_dataloader))
        test_predictions, test_targets, test_texts = generate_predictions(
            model, test_dataloader, tokenizer, DEVICE, num_samples=num_test_samples
        )
        final_metrics = evaluate_predictions(test_predictions, test_targets)

        for metric, value in final_metrics.items():
            print(f"{metric}: {value:.4f}")
    except Exception as e:
        print(f"Ошибка при оценке на тестовой выборке: {e}")
        # Создаем пустые метрики в случае ошибки
        final_metrics = {
            'bleu1': 0.0,
            'rouge1': 0.0,
            'rouge2': 0.0,
            'rougeL': 0.0,
            'word_match': 0.0
        }

    # Визуализация итоговых метрик
    plot_final_metrics(final_metrics)

    # Отображение примеров предсказаний
    print("\nПримеры предсказаний:")
    display_predictions(test_texts, test_predictions, test_targets, n=5)

    # Сохранение модели
    output_dir = './model_save/'
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    print(f"\nМодель и токенизатор сохранены в {output_dir}")

    return model, tokenizer, final_metrics

"""7. ЗАПУСК ОБУЧЕНИЯ И ОЦЕНКИ"""

from sklearn.model_selection import train_test_split
import pandas as pd

def load_and_prepare_data():
    # Загрузка данных
    try:
        csv_path = '/content/drive/MyDrive/balanced_legal_dataset.csv'
        df = pd.read_csv(csv_path)
        print(f"Данные успешно загружены из {csv_path}")
    except Exception as e:
        print(f"Ошибка при загрузке данных: {e}")
    # Разделение данных (добавлено)
    try:
        train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)
        val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)
        return train_df, val_df, test_df
    except Exception as split_error:
        print(f"Ошибка при разделении данных: {split_error}")
        return None, None, None

# 3. Повторное подключение после перезапуска
from google.colab import drive
drive.mount('/content/drive')

if __name__ == "__main__":
    print("\n Запуск обучения и оценки модели")
    model, tokenizer, metrics = train_and_evaluate()
    print("\nОбучение и оценка модели завершены успешно!")

