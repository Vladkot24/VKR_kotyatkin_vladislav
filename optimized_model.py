# -*- coding: utf-8 -*-
"""Optimized_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zmfQQpCQKAfW0pqwloQguZC6ZJfCTS8V

**Модель (Модель-экстратор) для извлечения ключевых мыслей из юридических текстов**

Импортируем все библиотеки
"""

# 1. Отключение (размонтирование) Google Drive
from google.colab import drive
drive.flush_and_unmount()

# 2. Перезапуск среды выполнения (отключение всех переменных и файлов)
import os
os.kill(os.getpid(), 9)

import pandas as pd
import re
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import os
from collections import Counter
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings("ignore")
from IPython.display import Image

"""Проверка датасета"""

# 3. Повторное подключение после перезапуска
from google.colab import drive
drive.mount('/content/drive')

df_check = pd.read_csv('/content/drive/MyDrive/new_legal_dataset.csv')
df_check.info()

"""Константы"""

MAX_SAMPLE_SIZE = 50000  # Число примеров для обработки
SEED = 42
OUTPUT_DIR = "/content/drive/MyDrive/model_results"  # Директория для сохранения результатов
os.makedirs(OUTPUT_DIR, exist_ok=True)

"""Функция для установки seed"""

def set_seed(seed=SEED):
    """Установка seed для воспроизводимости результатов"""
    np.random.seed(seed)

"""Поиск по регулярным выражениям"""

def extract_resolution(text):
    """
    Извлечение резолютивной части из судебного решения
    используя регулярные выражения и эвристики
    """
    if not isinstance(text, str):
        return ""

    # Очистка текста
    text = re.sub(r'\s+', ' ', text).strip().lower()

    # Расширенные регулярные выражения для поиска резолютивной части
    patterns = [
        # Удовлетворение/отказ в исковых требованиях
        r'(исков\w+ требовани\w+ .{3,80} (удовлетворить|отказать))',
        r'(в удовлетворении исков\w+ требовани\w+ .{3,150} отказать)',
        r'(удовлетворить исков\w+ требовани\w+ .{3,150})',
        r'(признать \w+ .{3,150} право собственности)',

        # Решения по административным делам
        r'(административн\w+ исков\w+ заявлени\w+ .{3,80} (удовлетворить|отказать))',
        r'(в удовлетворении административн\w+ исков\w+ .{3,100} отказать)',
        r'(административн\w+ иск\w* .{3,80} (удовлетворить|отказать))',

        # Решения по жалобам
        r'(апелляционн\w+ жалоб\w+ .{3,80} (удовлетворить|отказать|оставить без удовлетворения))',
        r'(кассационн\w+ жалоб\w+ .{3,80} (удовлетворить|отказать|оставить без удовлетворения))',
        r'(жалоб\w+ .{3,60} (удовлетворить|отказать|оставить без удовлетворения))',

        # Различные варианты формулировок решений
        r'(иск\w* удовлетворить( полностью)?)',
        r'(в иске отказать)',
        r'(заявлени\w+ удовлетворить( полностью)?)',
        r'(оставить без рассмотрения .{3,100})',
        r'(оставить без изменения .{3,100})',
        r'(производство по делу .{3,60} прекратить)',
        r'(взыскать с .{3,150} в пользу .{3,150})',
        r'((исков\w+ требовани\w+|иск\w+|заявлени\w+) удовлетворить частично .{3,150})',
        r'(решение суда .{3,60} оставить без изменения, .{3,100} жалобу без удовлетворения)',
        r'(определение суда .{3,60} оставить без изменения)',
        r'(признать .{3,150} (недействительным|незаконным))',
        r'(обязать .{3,150})',
        r'(расторгнуть .{3,150})',
        r'(приговорить .{3,150})',
        r'(возложить на .{3,100})',
        r'(апелляционн\w+ определение .{3,100} отменить)',
        r'( мировое соглашение)',
        r'(судебный приказ .{3,100} отменить)',
        r'(производств\w+ .{3,80} дел\w+ прекратить)'
    ]

    # Поиск по расширенным регулярным выражениям
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            # Проверка длины найденного текста
            result = match.group(1)
            if 10 <= len(result) <= 300:  # Разумное ограничение длины
                return result

    # Если ничего не найдено, поиск по ключевым словам и возврат предложения
    key_phrases = [
        "удовлетворить", "отказать", "оставить без рассмотрения",
        "оставить без изменения", "взыскать", "признать незаконным",
        "признать недействительным", "обязать", "производство по делу прекратить",
        "приговорить", "расторгнуть", "прекратить производство", "возложить на",
        "определение отменить", "приказ отменить"
    ]

    sentences = re.split(r'[.!?]', text)
    for sentence in sentences:
        sentence = sentence.strip()
        for phrase in key_phrases:
            if phrase in sentence and 10 <= len(sentence) <= 300:
                return sentence

    # Поиск решения по уголовным делам
    criminal_patterns = [
        r'(признать виновн\w+ .{3,150})',
        r'(назначить наказание .{3,150})',
        r'(приговорить к .{3,150})'
    ]

    for pattern in criminal_patterns:
        match = re.search(pattern, text)
        if match:
            result = match.group(1)
            if 10 <= len(result) <= 300:
                return result

    # Если ничего не найдено, возвращаем пустую строку
    return ""

"""Создание датасета с извлеченными резолютивными частями"""

def create_dataset_with_extracted_resolutions(csv_path, max_samples=MAX_SAMPLE_SIZE):
    """Создание датасета с извлеченными резолютивными частями"""
    print("Загрузка и обработка данных...")

    # Эффективная загрузка с использованием только нужных столбцов
    df = pd.read_csv(csv_path, usecols=['input_text', 'target_text'], nrows=max_samples)

    print(f"Загружено {len(df)} строк из CSV файла")

    # Очистка данных
    print("Предобработка текстов...")
    df = df.dropna(subset=['input_text', 'target_text'])
    df = df[(df['input_text'].astype(str) != "") & (df['target_text'].astype(str) != "")]

    print(f"После удаления пустых строк осталось {len(df)} строк")

    # Проверка наличия ключевых слов в текстах
    def check_for_key_terms(text):
        if not isinstance(text, str):
            return False
        text = text.lower()
        key_terms = ["удовлетворить", "отказать", "взыскать", "признать", "прекратить", "обязать", "расторгнуть"]
        return any(term in text for term in key_terms)

    key_terms_count = df['target_text'].apply(check_for_key_terms).sum()
    print(f"Текстов, содержащих ключевые слова решений: {key_terms_count} ({key_terms_count/len(df)*100:.2f}%)")

    # Извлечение резолютивной части
    print("Извлечение резолютивной части из текстов...")
    tqdm.pandas()  # Для отображения прогресса
    df['extracted_resolution'] = df['target_text'].progress_apply(extract_resolution)

    # Фильтрация строк, где не удалось извлечь резолютивную часть
    df_filtered = df[df['extracted_resolution'].astype(str).str.len() >= 10]

    print(f"После фильтрации строк с успешно извлеченной резолютивной частью осталось {len(df_filtered)} строк")
    print(f"Успешность извлечения: {len(df_filtered) / len(df) * 100:.2f}%")

    # Анализ категорий найденных резолютивных частей
    def categorize_resolution(resolution):
        resolution = resolution.lower()

        categories = {
            'Удовлетворение иска': ['иск удовлетворить', 'требовани удовлетворить', 'заявлени удовлетворить','удовлетворить'],
            'Отказ в иске': ['в иске отказать', 'в удовлетворении отказать', 'отказать'],
            'Частичное удовлетворение': ['частично удовлетворить', 'удовлетворить частично'],
            'Взыскание': ['взыскать с', 'взыскать в пользу','взыскать'],
            'Мировое соглашение':['мировое соглашение'],
            'Признание': ['признать недействительным', 'признать незаконным','признать'],
            'Оставление без изменения': ['оставить без изменения','оставить'],
            'Прекращение производства': ['производство прекратить', 'прекратить производство','прекратить'],
            'Отмена решения': ['решение отменить', 'определение отменить', 'приказ отменить','отменить'],
            'Уголовное наказание': ['признать виновн', 'приговорить к', 'назначить наказание','приговорить','наказание'],
            'Обязательство': ['обязать']
        }

        for category, patterns in categories.items():
            if any(pattern in resolution for pattern in patterns):
                return category

        return 'Другое'

    df_filtered['category'] = df_filtered['extracted_resolution'].apply(categorize_resolution)
    category_counts = df_filtered['category'].value_counts()

    print("\nРаспределение категорий резолютивных частей:")
    for category, count in category_counts.items():
        print(f"  • {category}: {count} ({count/len(df_filtered)*100:.2f}%)")

    # Сохранение обработанного датасета
    output_csv_path = f"{OUTPUT_DIR}/legal_dataset_with_resolutions.csv"
    df_filtered.to_csv(output_csv_path, index=False, encoding='utf-8')
    print(f"Датасет с извлеченными резолютивными частями сохранен в {output_csv_path}")

    # Визуализация успешности извлечения
    plt.figure(figsize=(8, 6))
    labels = ['Успешно извлечено', 'Не удалось извлечь']
    sizes = [len(df_filtered), len(df) - len(df_filtered)]
    colors = ['#66b3ff', '#ff9999']
    plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
    plt.axis('equal')
    plt.title('Результаты извлечения резолютивной части')

    # Сохранение графика
    pie_chart_path = f"{OUTPUT_DIR}/extraction_success_rate.png"
    plt.savefig(pie_chart_path)
    plt.close()

    # Визуализация категорий резолютивных частей
    plt.figure(figsize=(12, 8))
    category_data = category_counts.sort_values(ascending=False)
    category_data = category_data[:10]  # Топ-10 категорий для лучшей читаемости

    bars = plt.bar(category_data.index, category_data.values, color='#5DA5DA')
    plt.xticks(rotation=45, ha='right')
    plt.title('Распределение категорий резолютивных частей (топ-10)')
    plt.xlabel('Категория')
    plt.ylabel('Количество')
    plt.tight_layout()

    # Добавляем значения над столбцами
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                 f'{height}', ha='center', va='bottom')

    # Сохранение графика
    categories_chart_path = f"{OUTPUT_DIR}/resolution_categories.png"
    plt.savefig(categories_chart_path)
    plt.close()

    # Анализ данных для визуализации
    resolution_words = []
    for res in df_filtered['extracted_resolution'].astype(str).tolist():
        resolution_words.extend([word.lower() for word in res.split() if len(word) > 3])
    word_freq = Counter(resolution_words)

    # Визуализация частотности слов в резолютивной части
    plt.figure(figsize=(12, 6))
    common_words = dict(word_freq.most_common(20))
    plt.bar(common_words.keys(), common_words.values())
    plt.xticks(rotation=45, ha='right')
    plt.title('Частота слов в резолютивной части')
    plt.tight_layout()

    # Сохранение графика
    word_freq_path = f"{OUTPUT_DIR}/resolution_word_frequency.png"
    plt.savefig(word_freq_path)
    plt.close()

    # Визуализация распределения длин резолютивной части
    plt.figure(figsize=(10, 5))
    resolution_lengths = [len(res.split()) for res in df_filtered['extracted_resolution'].astype(str).tolist()]
    plt.hist(resolution_lengths, bins=20)
    plt.title('Распределение длин резолютивной части')
    plt.xlabel('Количество слов')
    plt.ylabel('Количество примеров')

    # Сохранение графика
    length_dist_path = f"{OUTPUT_DIR}/resolution_length_distribution.png"
    plt.savefig(length_dist_path)
    plt.close()

    # Вместо разбиения на тренировочную и валидационную выборки
    # просто сохраняем единый датасет с извлеченными резолютивными частями
    print(f"Датасет содержит {len(df_filtered)} примеров с успешно извлеченной резолютивной частью")

    # Показать примеры извлеченных резолютивных частей
    print("\nПримеры извлеченных резолютивных частей:")
    sample_df = df_filtered.sample(n=min(5, len(df_filtered)), random_state=SEED)

    for i, row in enumerate(sample_df.itertuples(), 1):
        print(f"\nПример {i}:")
        print(f"Оригинальный текст (начало): {row.target_text[:100]}...")
        print(f"Извлеченная резолютивная часть: {row.extracted_resolution}")
        print("-" * 80)

    # Показать примеры, где не удалось извлечь резолютивную часть
    print("\nПримеры, где НЕ удалось извлечь резолютивную часть:")
    df_failed = df[df['extracted_resolution'].astype(str).str.len() < 10]

    # Анализ текстов, в которых не удалось найти резолютивную часть
    def analyze_text_for_potential_patterns(text):
        if not isinstance(text, str):
            return []

        text = text.lower()
        # Список ключевых слов, которые могут указывать на резолютивную часть
        potential_keywords = [
            "решил", "постановил", "определил", "приказываю",
            "удовлетворить", "отказать", "прекратить", "взыскать",
            "признать", "приговорить", "освободить", "обязать",
            "назначить", "прекратить производство", "оставить без рассмотрения"
        ]

        # Проверяем наличие ключевых слов и собираем контекст
        found_contexts = []
        for keyword in potential_keywords:
            if keyword in text:
                # Находим позицию ключевого слова
                pos = text.find(keyword)
                # Извлекаем контекст (до 50 символов до и 100 символов после ключевого слова)
                start = max(0, pos - 50)
                end = min(len(text), pos + 100)
                context = text[start:end]
                found_contexts.append((keyword, context))

        return found_contexts

    # Анализируем неудачные извлечения
    failed_samples = df_failed.sample(n=min(5, len(df_failed)), random_state=SEED)

    for i, row in enumerate(failed_samples.itertuples(), 1):
        print(f"\nПример неудачного извлечения {i}:")
        print(f"Оригинальный текст (начало 200 символов):\n{row.target_text[:200]}...")

        # Анализируем текст для выявления потенциальных паттернов
        potential_patterns = analyze_text_for_potential_patterns(row.target_text)

        if potential_patterns:
            print("\nПотенциальные фрагменты резолютивной части:")
            for keyword, context in potential_patterns:
                print(f"  • Ключевое слово '{keyword}': '...{context}...'")
        else:
            print("\nВ тексте не найдены типичные ключевые слова резолютивной части.")

        print("-" * 80)

    return df_filtered

"""Запуск"""

# Установка seed для воспроизводимости
set_seed()

# Путь к файлу с данными
csv_path = '/content/drive/MyDrive/new_legal_dataset.csv'

import shutil
import os

# Если директория существует и не пуста — удалим её
# mount_path = '/content/drive'
# if os.path.isdir(mount_path) and os.listdir(mount_path):
#    shutil.rmtree(mount_path)

# Создание датасета с извлеченными резолютивными частями
dataset = create_dataset_with_extracted_resolutions(csv_path, max_samples=MAX_SAMPLE_SIZE)

print("\nОбработка завершена. Новый датасет создан и сохранен.")

"""Выводим визиуально результаты"""

image_path = '/content/drive/MyDrive/model_results/extraction_success_rate.png'

Image(filename=image_path, width=1000)

image_path = '/content/drive/MyDrive/model_results/resolution_categories.png'

Image(filename=image_path, width=1000)

image_path = '/content/drive/MyDrive/model_results/resolution_word_frequency.png'

Image(filename=image_path, width=1000)

image_path = '/content/drive/MyDrive/model_results/resolution_length_distribution.png'

Image(filename=image_path, width=1000)

"""Балансировка данных"""

def balance_dataset_median(df, target_column='category', seed=42):
    """
    Балансирует датасет до медианного размера категорий.
    Категории с количеством примеров больше медианы - недостовыборка.
    Категории с количеством примеров меньше медианы - перевыборка с повторением.
    """
    print(f"Балансировка датасета по колонке '{target_column}'...")

    # Получаем размеры всех категорий
    category_sizes = df[target_column].value_counts()
    print("\nРазмеры категорий до балансировки:")
    for category, size in category_sizes.items():
        print(f"  • {category}: {size}")

    # Вычисляем медианный размер
    median_size = int(category_sizes.median())
    print(f"\nМедианный размер категории: {median_size}")

    balanced_dfs = []

    for category in df[target_column].unique():
        category_df = df[df[target_column] == category]
        category_size = len(category_df)

        # Для категорий больше медианного - недостовыборка
        # Для категорий меньше медианного - перевыборка (с повторениями)
        resampled_df = resample(
            category_df,
            replace=(category_size < median_size),  # Если меньше медианы, разрешаем повторения
            n_samples=median_size,
            random_state=seed
        )
        balanced_dfs.append(resampled_df)

        action = "перевыборка" if category_size < median_size else "недостовыборка"
        print(f"  • {category}: {action} с {category_size} до {median_size}")

    # Объединяем все сбалансированные части и перемешиваем
    balanced_df = pd.concat(balanced_dfs).sample(frac=1, random_state=seed)
    print(f"\nРазмер сбалансированного датасета: {len(balanced_df)} примеров")

    return balanced_df

def visualize_balanced_dataset(original_df, balanced_df, output_dir):
    """
    Визуализация результатов балансировки и статистик по сбалансированному датасету
    """
    print("Создание визуализаций для сбалансированного датасета...")

    # 1. Сравнение распределения категорий до и после балансировки
    plt.figure(figsize=(14, 8))

    # До балансировки
    plt.subplot(1, 2, 1)
    original_counts = original_df['category'].value_counts().sort_values(ascending=False)
    bars1 = plt.bar(original_counts.index, original_counts.values, color='#5DA5DA')
    plt.xticks(rotation=45, ha='right')
    plt.title('Распределение категорий до балансировки')
    plt.xlabel('Категория')
    plt.ylabel('Количество')

    # После балансировки
    plt.subplot(1, 2, 2)
    balanced_counts = balanced_df['category'].value_counts().sort_values(ascending=False)
    bars2 = plt.bar(balanced_counts.index, balanced_counts.values, color='#FAA43A')
    plt.xticks(rotation=45, ha='right')
    plt.title('Распределение категорий после балансировки')
    plt.xlabel('Категория')
    plt.ylabel('Количество')

    plt.tight_layout()

    # Сохранение графика
    balance_comparison_path = f"{output_dir}/category_balance_comparison.png"
    plt.savefig(balance_comparison_path)
    plt.close()
    print(f"График сравнения категорий сохранен: {balance_comparison_path}")

    # 2. Анализ частотности слов в сбалансированном датасете
    resolution_words = []
    for res in balanced_df['extracted_resolution'].astype(str).tolist():
        resolution_words.extend([word.lower() for word in res.split() if len(word) > 3])
    word_freq = Counter(resolution_words)

    plt.figure(figsize=(12, 6))
    common_words = dict(word_freq.most_common(20))
    plt.bar(common_words.keys(), common_words.values(), color='#60BD68')
    plt.xticks(rotation=45, ha='right')
    plt.title('Частота слов в резолютивной части (сбалансированный датасет)')
    plt.tight_layout()

    # Сохранение графика
    balanced_word_freq_path = f"{output_dir}/balanced_resolution_word_frequency.png"
    plt.savefig(balanced_word_freq_path)
    plt.close()
    print(f"График частоты слов сохранен: {balanced_word_freq_path}")

    # 3. Распределение длин резолютивной части в сбалансированном датасете
    plt.figure(figsize=(10, 5))
    resolution_lengths = [len(res.split()) for res in balanced_df['extracted_resolution'].astype(str).tolist()]
    plt.hist(resolution_lengths, bins=20, color='#F17CB0')
    plt.title('Распределение длин резолютивной части (сбалансированный датасет)')
    plt.xlabel('Количество слов')
    plt.ylabel('Количество примеров')

    # Сохранение графика
    balanced_length_dist_path = f"{output_dir}/balanced_resolution_length_distribution.png"
    plt.savefig(balanced_length_dist_path)
    plt.close()
    print(f"График распределения длин сохранен: {balanced_length_dist_path}")

    # 4. Перекрестная визуализация: распределение длин резолютивной части по категориям
    plt.figure(figsize=(14, 8))

    # Создаем словарь для хранения длин резолютивных частей по категориям
    category_lengths = {}
    for category in balanced_df['category'].unique():
        category_df = balanced_df[balanced_df['category'] == category]
        lengths = [len(res.split()) for res in category_df['extracted_resolution'].astype(str).tolist()]
        category_lengths[category] = lengths

    # Создаем boxplot
    plt.boxplot([lengths for lengths in category_lengths.values()],
                labels=category_lengths.keys(),
                patch_artist=True)
    plt.xticks(rotation=45, ha='right')
    plt.title('Распределение длин резолютивной части по категориям')
    plt.xlabel('Категория')
    plt.ylabel('Количество слов')
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.tight_layout()

    # Сохранение графика
    category_length_path = f"{output_dir}/category_length_distribution.png"
    plt.savefig(category_length_path)
    plt.close()
    print(f"График распределения длин по категориям сохранен: {category_length_path}")

    return [balance_comparison_path, balanced_word_freq_path, balanced_length_dist_path, category_length_path]

"""Запуск балансировки"""

from sklearn.utils import resample

balanced_dataset = balance_dataset_median(dataset, target_column='category', seed=SEED)

"""Сохраняем новый сбалансированный датасет"""

output_path_drive = '/content/drive/MyDrive/balanced_legal_dataset.csv'
balanced_dataset.to_csv(output_path_drive, index=False, encoding='utf-8-sig')
print(f"Сохранено на Google Диск: {output_path_drive}")

